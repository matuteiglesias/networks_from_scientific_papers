{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of UN migration dataset\n",
    "### We use migration data of 2013, both genders. See [document](http://www.un.org/en/development/desa/population/migration/data/estimates/docs/OriginMIgrantStocks_Documentation.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import binet as bt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries data\n",
    "### GDP PC and GDP PC growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File CountryData2.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-13697baf342b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# c_table1 = pd.read_csv('CountryData1.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# c_table1 = c_table1.rename(columns={'Country code': 'M49'})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mc_table2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CountryData2.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mc_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_table1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_table2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Country or area'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ISO3166-1-Alpha-2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ISO3166-1-Alpha-3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Major area'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Region'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Capital'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mc_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ccode'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ISO3166-1-Alpha-3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/miglesia/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/miglesia/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/miglesia/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/miglesia/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/miglesia/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File CountryData2.txt does not exist"
     ]
    }
   ],
   "source": [
    "#Country facts\n",
    "c_table1 = pd.read_csv('CountryData1.csv')\n",
    "c_table1 = c_table1.rename(columns={'Country code': 'M49'})\n",
    "c_table2 = pd.read_table('CountryData2.txt', delimiter=',')\n",
    "c_table = c_table1.merge(c_table2)[['Country or area','ISO3166-1-Alpha-2','ISO3166-1-Alpha-3','Major area','Region','Capital']]\n",
    "c_table['ccode'] = c_table['ISO3166-1-Alpha-3'].str.lower()\n",
    "c_table = c_table.drop('ISO3166-1-Alpha-3', 1)\n",
    "\n",
    "#GDP per capita, current US$.\n",
    "GDPPC = pd.read_csv('GDPPC-countries.csv')\n",
    "GDPPC = GDPPC[['Country','2002','2003', '2004', '2005', '2006', '2007', '2008',\n",
    "               '2009', '2010', '2011', '2012', '2013','2014']].dropna()\n",
    "\n",
    "#Build period-mean GDPPC dataset\n",
    "pmGDP = pd.DataFrame()\n",
    "\n",
    "for y in [2003, 2006, 2009, 2012]:\n",
    "    GDPPC_ = pd.concat([GDPPC.Country, GDPPC[[str(y),str(y + 1),str(y + 2)]].mean(axis = 1)],axis = 1)\n",
    "    GDPPC_ = GDPPC_.rename(columns={0: 'GDPPC_mean_'+str(y)})\n",
    "    if pmGDP.empty:\n",
    "        pmGDP = pmGDP.append(GDPPC_)\n",
    "    else:\n",
    "        pmGDP = pmGDP.merge(GDPPC_)\n",
    "\n",
    "pmGDP = pmGDP.rename(columns={'Country': 'Country or area'})\n",
    "c_table = c_table.merge(pmGDP)\n",
    "\n",
    "#Build period GDPPC growth dataset\n",
    "pmGDPgrowth = pd.DataFrame()\n",
    "\n",
    "for y in [2003, 2006, 2009, 2012]:\n",
    "    GDPPCgrowth_ = pd.concat([GDPPC.Country, GDPPC[str(y + 2)].divide(GDPPC[str(y - 1)]) - 1],axis = 1)\n",
    "    GDPPCgrowth_ = GDPPCgrowth_.rename(columns={0: 'GDPPC_tot_growth_'+str(y)})\n",
    "    if pmGDPgrowth.empty:\n",
    "        pmGDPgrowth = pmGDPgrowth.append(GDPPCgrowth_)\n",
    "    else:\n",
    "        pmGDPgrowth = pmGDPgrowth.merge(GDPPCgrowth_)\n",
    "\n",
    "pmGDPgrowth = pmGDPgrowth.rename(columns={'Country': 'Country or area'})\n",
    "c_table = c_table.merge(pmGDPgrowth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Coords of capital cities for plotting\n",
    "capital_coords = pd.read_csv('CountryCapitals.csv')[['CapitalLatitude', 'CapitalLongitude', 'CountryCode']]\n",
    "capital_coords = capital_coords.rename(columns={'CountryCode': 'ISO3166-1-Alpha-2','CapitalLongitude': 'lng','CapitalLatitude': 'lat'})\n",
    "c_table = c_table.merge(capital_coords, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "wt, pr, co = bt.trade_data('hs02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = pd.DataFrame()\n",
    "\n",
    "for y in [2003, 2006, 2009, 2012]:\n",
    "    data = wt[wt['year'].between(y,y + 2)][['ccode','pcode','x']].groupby(['ccode','pcode']).sum()[['x']].reset_index()\n",
    "    M = bt.mcp(data=data,c='ccode',p='pcode')\n",
    "    ECI,PCI = M.CalculateComplexity()\n",
    "    ECI = ECI.rename(columns={'ECI': 'ECI_'+str(y)})\n",
    "    if comp.empty:\n",
    "        comp = comp.append(ECI)\n",
    "    else:\n",
    "        comp = comp.merge(ECI)\n",
    "comp[['ECI_2003_rank','ECI_2006_rank','ECI_2009_rank','ECI_2012_rank']] = comp.drop('ccode', axis = 1).rank(ascending = False).astype(int)\n",
    "\n",
    "c_table = c_table.merge(comp)\n",
    "\n",
    "c_table = c_table.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network stats\n",
    "### From migration data\n",
    "#### Here we can introduce a weighting variable. For example, what if we weight inmigration by GDP PC of country of origin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Main dataset\n",
    "migration2013 = pd.read_csv('migrationUN2013.csv')\n",
    "migration2010 = pd.read_csv('migrationUN2010.csv')\n",
    "\n",
    "def cleanup_migrationUN(df):\n",
    "    \"\"\"\n",
    "    Clean up to give it the shape of an adjacency matrix (ideal for network statistics)\n",
    "    \"\"\"\n",
    "    df = df.loc[df['Major area, region, country or area of destination'].isin(c_table['Country or area'])]\n",
    "    df = df.set_index('Major area, region, country or area of destination')\n",
    "    df = df[c_table['Country or area']]\n",
    "\n",
    "    for col in df.columns[df.dtypes == object]:\n",
    "        df[col] = df[col].str.replace(\" \",\"\")\n",
    "    df = df.fillna(0)\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "migration2010 = cleanup_migrationUN(migration2010)\n",
    "migration2013 = cleanup_migrationUN(migration2013)\n",
    "\n",
    "def weight_columns(df, series):\n",
    "    \"\"\"\n",
    "    Multiply columns (inmigrants in UN migration datasets) with some variable.\n",
    "    \n",
    "    Example:\n",
    "        migration2010_w = weight_columns(migration2010, c_table['GDPPC_mean_2012'])\n",
    "    \n",
    "    \"\"\"\n",
    "    df_w = df.apply(lambda x: np.asarray(x, dtype = int) * np.asarray(series\n",
    "                                                                  , dtype = int), axis=1)\n",
    "    return df_w\n",
    "\n",
    "migration2010_w = weight_columns(migration2010, c_table['GDPPC_mean_2012'])\n",
    "migration2013_w = weight_columns(migration2013, c_table['GDPPC_mean_2012'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that computes a bunch of network statistics out of the adjacency matrix dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nw_stats_from_adj_dataframe(df, label = ''):\n",
    "    adj_mat = df.as_matrix().astype(int)\n",
    "    G = nx.from_numpy_matrix(adj_mat, create_using=nx.DiGraph())\n",
    "    nx.set_node_attributes(G, 'Country or area', dict(enumerate(df.index)))\n",
    "    nx.set_node_attributes(G, 'degree_centrality'+label, nx.degree_centrality(G))\n",
    "    nx.set_node_attributes(G, 'closeness_centrality'+label, nx.closeness_centrality(G))\n",
    "    nx.set_node_attributes(G, 'betweenness_centrality'+label, nx.betweenness_centrality(G, weight='weight'))\n",
    "    nx.set_node_attributes(G, 'eigenvector_centrality'+label, nx.eigenvector_centrality(G, weight='weight'))\n",
    "\n",
    "    centralities = pd.DataFrame([r[1] for r in G.nodes(data = True)])\n",
    "\n",
    "    centralities['avg_cent_rank'+label] = centralities.drop('Country or area', axis=1).rank(ascending = False).astype(int).mean(axis = 1)\n",
    "    # centralities.sort_values(by = 'avg_cent_rank')\n",
    "    \n",
    "    nx.set_node_attributes(G,  'in_size'+label,  G.in_degree(weight='weight'))\n",
    "    nx.set_node_attributes(G, 'out_size'+label, G.out_degree(weight='weight'))\n",
    "\n",
    "    sizes = pd.DataFrame([r[1] for r in G.nodes(data = True)])[['Country or area','in_size'+label,'out_size'+label]]\n",
    "    sizes[['in_size_rank'+label,'out_size_rank'+label]] = sizes[['in_size'+label,'out_size'+label]].rank(ascending = False).astype(int)\n",
    "\n",
    "    stats = centralities.merge(sizes)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compute unweighted and weighted network statistics. \n",
    "stats2010 = nw_stats_from_adj_dataframe(migration2010, label = '_2010')\n",
    "stats2010_w = nw_stats_from_adj_dataframe(migration2010_w, label = '_2010GDPw')\n",
    "\n",
    "stats2013 = nw_stats_from_adj_dataframe(migration2013, label = '_2013')\n",
    "stats2013_w = nw_stats_from_adj_dataframe(migration2013_w, label = '_2013GDPw')\n",
    "\n",
    "stats = pd.concat([stats2010, stats2010_w, stats2013, stats2013_w], axis = 1).T.drop_duplicates().T\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore');\n",
    "\n",
    "stats['betweenness_centrality_2013'] = np.log10(stats['betweenness_centrality_2013'].astype(float))\n",
    "stats['eigenvector_centrality_2013'] = np.log10(stats['eigenvector_centrality_2013'].astype(float))\n",
    "stats['in_size_2013'] = np.log10(stats['in_size_2013'].astype(float))\n",
    "stats['out_size_2013'] = np.log10(stats['out_size_2013'].astype(float))\n",
    "stats['betweenness_centrality_2013GDPw'] = np.log10(stats['betweenness_centrality_2013GDPw'].astype(float))\n",
    "stats['eigenvector_centrality_2013GDPw'] = np.log10(stats['eigenvector_centrality_2013GDPw'].astype(float))\n",
    "stats['in_size_2013GDPw'] = np.log10(stats['in_size_2013GDPw'].astype(float))\n",
    "stats['out_size_2013GDPw'] = np.log10(stats['out_size_2013GDPw'].astype(float))\n",
    "\n",
    "# Export the resulting table\n",
    "# stats.to_csv('MigrationNwStats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare variables in to be used as X and Y among them, so to select a meaningful subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def main():\n",
    "    data = stats.as_matrix().T[17:]\n",
    "    numvars, numdata = data.shape\n",
    "    fig = scatterplot_matrix(data, list(stats.columns[17:]),\n",
    "            linestyle='none', marker='.', color='black', mfc='none')\n",
    "#     fig.suptitle('')\n",
    "    plt.show()\n",
    "\n",
    "def scatterplot_matrix(data, names, **kwargs):\n",
    "    \"\"\"Plots a scatterplot matrix of subplots.  Each row of \"data\" is plotted\n",
    "    against other rows, resulting in a nrows by nrows grid of subplots with the\n",
    "    diagonal subplots labeled with \"names\".  Additional keyword arguments are\n",
    "    passed on to matplotlib's \"plot\" command. Returns the matplotlib figure\n",
    "    object containg the subplot grid.\"\"\"\n",
    "    numvars, numdata = data.shape\n",
    "    fig, axes = plt.subplots(nrows=numvars, ncols=numvars, figsize=(numvars,numvars))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        # Hide all ticks and labels\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        # Set up ticks only on one side for the \"edge\" subplots...\n",
    "        if ax.is_first_col():\n",
    "            ax.yaxis.set_ticks_position('left')\n",
    "        if ax.is_last_col():\n",
    "            ax.yaxis.set_ticks_position('right')\n",
    "        if ax.is_first_row():\n",
    "            ax.xaxis.set_ticks_position('top')\n",
    "        if ax.is_last_row():\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    # Plot the data.\n",
    "    for i, j in zip(*np.triu_indices_from(axes, k=1)):\n",
    "        for x, y in [(i,j), (j,i)]:\n",
    "            axes[x,y].plot(data[x], data[y], **kwargs)\n",
    "\n",
    "    # Label the diagonal subplots...\n",
    "    for i, label in enumerate(names):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), xycoords='axes fraction',\n",
    "                ha='center', va='center')\n",
    "\n",
    "    # Turn on the proper x or y axes ticks.\n",
    "    for i, j in zip(range(numvars), itertools.cycle((-1, 0))):\n",
    "        axes[j,i].xaxis.set_visible(True)\n",
    "        axes[i,j].yaxis.set_visible(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph (network) and export to plot it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_mat = df_w.as_matrix().astype(int) #adjacency matrix\n",
    "G = nx.from_numpy_matrix(adj_mat, create_using=nx.DiGraph())\n",
    "\n",
    "#create list and then dicts with node attributes: name/label, longitude and latitude\n",
    "node_name = {}; node_lng = {}; node_lat = {};\n",
    "for index, row in c_table.iterrows():\n",
    "    node_name[index] = row['ISO3166-1-Alpha-2']\n",
    "    node_lng[index] = row['lng']\n",
    "    node_lat[index] = row['lat']\n",
    "\n",
    "node_name = {Urc_key: node_name[Urc_key] for Urc_key in G.nodes()}\n",
    "node_lng = {Urc_key: node_lng[Urc_key] for Urc_key in G.nodes()}\n",
    "node_lat = {Urc_key: node_lat[Urc_key] for Urc_key in G.nodes()}\n",
    "\n",
    "#set the attributes to the graph nodes\n",
    "nx.set_node_attributes(G, 'Label', node_name)\n",
    "nx.set_node_attributes(G, 'lng', node_lng)\n",
    "nx.set_node_attributes(G, 'lat', node_lat)\n",
    "\n",
    "#export as .gexf\n",
    "nx.write_gexf(G, 'migrationnw_w.gexf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
